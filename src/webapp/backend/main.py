from fastapi import FastAPI
from pydantic import BaseModel
from typing import Literal, List, Optional
import random
import logging
from fastapi.responses import JSONResponse
from fastapi import Request
import sys
import os


# Configure logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stdout,
    force=True  # only works in Python 3.8+
)
logger = logging.getLogger(__name__)

app = FastAPI(title="Sentiment API")
from fastapi.middleware.cors import CORSMiddleware
import sys
import os
from scrapper.youtube import get_video_id, fetch_comments, getTopComments
from sentiment_analyzer import analyze_youtube_comments_with_model, initialize_stella_model
import csv
from datetime import datetime

# Add this right after creating the FastAPI app
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # You can restrict this to your frontend URL for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
async def startup_event():
    """
    Initialise le mod√®le Stella au d√©marrage du serveur
    """
    logger.info("üöÄ D√©marrage du serveur - Initialisation du mod√®le Stella...")
    if initialize_stella_model():
        logger.info("‚úÖ Mod√®le Stella initialis√© avec succ√®s")
    else:
        logger.error("‚ùå √âchec de l'initialisation du mod√®le Stella")

# === URL input schema ===
class URLRequest(BaseModel):
    url: str
    model: str = "mvp"  # Par d√©faut, utilise le mod√®le MVP

# === Normalized Comment schema ===
class NormalizedComment(BaseModel):
    id: int
    text: str
    label: str
    probability: float

# === Comment schema for frontend ===
class Comment(BaseModel):
    text: str
    sentiment: Literal["positive", "neutral", "negative"]
    likes: int
    time: str  # e.g., "12:34"

# === Response for detailed analysis ===
class DetailedAnalysisResponse(BaseModel):
    totalComments: int
    positive: float
    negative: float
    neutral: float
    comments: List[Comment]
    emotion_counts: Optional[dict] = None  # Ajout d'un champ optional pour les √©motions

async def generate_analysis(url, model_name="mvp") -> DetailedAnalysisResponse:
    """
    G√©n√®re une analyse de sentiment √† partir des commentaires YouTube.
    """
    logger.debug(f"D√©marrage de l'analyse des commentaires YouTube avec le mod√®le {model_name}")

    try:
        video_id = get_video_id(url)
        default_csv_path = os.path.join(
            os.path.dirname(os.path.abspath(__file__)),
            "scrapper", "output", f"youtube_comments_{video_id}.csv"
        )

        csv_file_path = default_csv_path
        top_comments = []

        # üîç V√©rifier si le fichier CSV existe d√©j√†
        if os.path.exists(csv_file_path):
            logger.info(f"Fichier CSV existant trouv√© : {csv_file_path}")
        else:
            # üì• Si le fichier n'existe pas, appeler l'API pour r√©cup√©rer les commentaires
            logger.info(f"Fichier CSV non trouv√©, r√©cup√©ration des commentaires depuis YouTube")
            result = await getCommentsFromYoutube(url)

            # Mettre √† jour les chemins et commentaires depuis les r√©sultats
            csv_file_path = result.get("file_path", default_csv_path)
            top_comments = result.get("comments", [])

        # üî¢ Compter les commentaires
        total_comments = len(top_comments)
        if os.path.exists(csv_file_path):
            with open(csv_file_path, 'r', encoding='utf-8') as file:
                reader = csv.reader(file)
                row_count = sum(1 for row in reader) - 1
                if row_count > 0:
                    total_comments = row_count
                    logger.debug(f"Nombre total de commentaires dans le CSV: {total_comments}")

        # Analyser les commentaires avec le mod√®le sp√©cifi√©
        try:
            analysis_results = analyze_youtube_comments_with_model(csv_file_path, model_name)
            
            if "error" in analysis_results:
                logger.error(f"Erreur d'analyse: {analysis_results['error']}")
                raise ValueError(analysis_results["error"])

        except Exception as e:
            logger.error(f"Exception lors de l'analyse: {str(e)}")
            return DetailedAnalysisResponse(
                totalComments=total_comments,
                positive=0.33,
                negative=0.33,
                neutral=0.34,
                comments=[Comment(
                    text=f"Erreur d'analyse: {str(e)}",
                    sentiment="neutral",
                    likes=0,
                    time="00:00"
                )]
            )

        
        # V√©rifier que nous avons bien des r√©sultats normalis√©s
        if "normalized_results" not in analysis_results or not analysis_results["normalized_results"]:
            logger.warning("Aucun commentaire n'a pu √™tre analys√©")
            return DetailedAnalysisResponse(
                totalComments=total_comments,
                positive=0.33,
                negative=0.33,
                neutral=0.34,
                comments=[Comment(
                    text="Aucun commentaire n'a pu √™tre analys√© avec succ√®s.",
                    sentiment="neutral",
                    likes=0,
                    time="00:00"
                )]
            )
        
        # R√©cup√©rer les r√©sultats normalis√©s
        normalized_results = analysis_results["normalized_results"]
        logger.info(f"Commentaires analys√©s avec succ√®s: {len(normalized_results)}")
        
        # Log pour v√©rifier les d√©comptes d'√©motions
        if "emotion_counts" in analysis_results:
            emotion_counts = analysis_results["emotion_counts"]
            logger.info(f"Emotions d√©tect√©es: {emotion_counts}")
            
            # V√©rifier la pr√©sence de gratitude
            if "gratitude" in emotion_counts:
                logger.info(f"‚úÖ Gratitude d√©tect√©e avec {emotion_counts['gratitude']} occurrences")
            else:
                logger.warning("‚ö†Ô∏è Aucune gratitude d√©tect√©e dans les √©motions!")
        else:
            logger.warning("‚ö†Ô∏è Aucun d√©compte d'√©motions (emotion_counts) dans les r√©sultats d'analyse!")
        
        # R√©cup√©rer les statistiques de sentiment
        positive_ratio = analysis_results['sentiment_percentages']['positive'] / 100
        negative_ratio = analysis_results['sentiment_percentages']['negative'] / 100
        neutral_ratio = analysis_results['sentiment_percentages']['neutral'] / 100
        
        # Cr√©er un dictionnaire pour mapper les commentaires avec leurs m√©tadonn√©es (likes, time)
        comment_metadata = {item.get("text", ""): item for item in top_comments}
        
        # Convertir les r√©sultats normalis√©s en objets Comment pour le frontend
        comments_for_frontend = []
        
        # D√©finir les cat√©gories d'√©motions
        positive_emotions = ["joy", "admiration", "amusement", "excitement", "gratitude", "love", "optimism", "pride", "relief", "approval", "caring"]
        negative_emotions = ["anger", "annoyance", "disappointment", "disapproval", "disgust", "embarrassment", "fear", "grief", "remorse", "sadness", "confusion", "nervousness"]
        
        # Limite √† 30 commentaires pour l'affichage
        for result in normalized_results[:30]:
            # D√©terminer le sentiment g√©n√©ral √† partir de l'√©motion
            label = result["label"]
            sentiment = "neutral"
            
            if label in positive_emotions:
                sentiment = "positive"
            elif label in negative_emotions:
                sentiment = "negative"
            
            # R√©cup√©rer les m√©tadonn√©es si disponibles
            text = result["text"]
            metadata = comment_metadata.get(text, {})
            
            comments_for_frontend.append(Comment(
                text=text,
                sentiment=sentiment,
                likes=metadata.get("likes", 0),
                time=metadata.get("time", "00:00")
            ))
        
        response = DetailedAnalysisResponse(
            totalComments=total_comments,
            positive=round(positive_ratio, 2),
            negative=round(negative_ratio, 2),
            neutral=round(neutral_ratio, 2),
            comments=comments_for_frontend,
            emotion_counts=analysis_results.get("emotion_counts", {})
        )
        
        logger.debug("Analyse compl√©t√©e avec succ√®s")
        return response

    except Exception as e:
        logger.error(f"Erreur lors de l'analyse: {str(e)}", exc_info=True)
        
        # Retourner une r√©ponse d'erreur structur√©e
        return DetailedAnalysisResponse(
            totalComments=0,
            positive=0.0,
            negative=0.0,
            neutral=1.0,
            comments=[Comment(
                text=f"Erreur: {str(e)}",
                sentiment="neutral",
                likes=0,
                time="00:00"
            )]
        )

@app.exception_handler(Exception)
async def exception_handler(request: Request, exc: Exception):
    import traceback
    return JSONResponse(
        status_code=500,
        content={
            "error": str(exc),
            "traceback": traceback.format_exc()
        }
    )

# === Endpoint for YouTube analysis ===
@app.post("/analyze/youtube", response_model=DetailedAnalysisResponse)
async def analyze_youtube(request: URLRequest):
    """
    Analyse les commentaires d'une vid√©o YouTube et retourne une analyse d√©taill√©e.
    """
    try:
        return await generate_analysis(request.url, request.model)
    except Exception as e:
        logger.error(f"Erreur lors de l'analyse YouTube: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# === D√©finition du mod√®le pour les commentaires normalis√©s (pour une API future) ===
class NormalizedCommentsResponse(BaseModel):
    total_comments: int
    processed_comments: int
    skipped_comments: int
    emotion_counts: dict
    normalized_results: List[NormalizedComment]

# === Endpoint pour obtenir les commentaires au format normalis√© ===
@app.post("/analyze/youtube/normalized", response_model=NormalizedCommentsResponse)
async def analyze_youtube_normalized(request: URLRequest):
    """
    Analyse les commentaires YouTube et retourne les r√©sultats au format normalis√©
    (id, texte, label, probabilit√©) sans les convertir pour le frontend.
    Cette API est utile pour les cas d'utilisation avanc√©s ou l'int√©gration avec d'autres syst√®mes.
    """
    logger.info(f"Analyse normalis√©e demand√©e pour URL: {request.url}")
    try:
        # R√©cup√©rer les commentaires YouTube
        result = await getCommentsFromYoutube(url=request.url)
        csv_file_path = result.get("file_path", "")
        
        # Analyser les commentaires
        analysis_results = analyze_youtube_comments_with_model(csv_file_path)
        
        if "error" in analysis_results:
            raise ValueError(analysis_results["error"])
        
        # Cr√©er la r√©ponse normalis√©e
        normalized_response = NormalizedCommentsResponse(
            total_comments=analysis_results["total_comments"],
            processed_comments=analysis_results["processed_comments"],
            skipped_comments=analysis_results["skipped_comments"] if "skipped_comments" in analysis_results else 0,
            emotion_counts=analysis_results["emotion_counts"],
            normalized_results=[
                NormalizedComment(
                    id=item["id"],
                    text=item["text"],
                    label=item["label"],
                    probability=item["probability"]
                ) for item in analysis_results["normalized_results"]
            ]
        )
        
        logger.info("Analyse normalis√©e compl√©t√©e avec succ√®s")
        return normalized_response
        
    except Exception as e:
        logger.error(f"Erreur lors de l'analyse normalis√©e: {str(e)}", exc_info=True)
        raise

async def getCommentsFromYoutube(url):
    """
    R√©cup√®re les commentaires d'une vid√©o YouTube.
    """
    try:
        logger.debug(f"Tentative de r√©cup√©ration des commentaires YouTube pour URL: {url}")
        
        # Obtenir l'ID de la vid√©o et d√©clencher la r√©cup√©ration des commentaires
        video_id = get_video_id(url)
        csv_file_path = fetch_comments(url)
        
        topComments = getTopComments(url)
        logger.debug(f"Commentaires r√©cup√©r√©s pour la vid√©o ID: {video_id}")
        
        return {
            "status": "success", 
            "message": "R√©cup√©ration des commentaires r√©ussie", 
            "file_path": csv_file_path, 
            "comments": topComments
        }
    
    except ImportError as e:
        logger.error(f"√âchec d'importation du scraper YouTube: {str(e)}")
        raise
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des commentaires YouTube: {str(e)}")
        raise

@app.get("/comments/{emotion}", response_model=None)
async def get_comments_by_emotion(emotion: str, video_id: str = None):
    """
    R√©cup√®re les commentaires pour une √©motion sp√©cifique √† partir des fichiers CSV.
    
    Args:
        emotion (str): L'√©motion √† rechercher dans les commentaires
        video_id (str, optional): L'ID de la vid√©o YouTube pour filtrer les fichiers sp√©cifiques
    """
    logger.info(f"Requ√™te de commentaires pour l'√©motion: {emotion}{' et la vid√©o: ' + video_id if video_id else ''}")
    
    try:
        # Trouver les fichiers CSV dans le r√©pertoire de sortie
        csv_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "scrapper", "output")
        csv_files = [f for f in os.listdir(csv_dir) if f.endswith('_labeled.csv') or f.endswith('_labeled_20') or 'labeled' in f]
        
        if not csv_files:
            logger.warning("Aucun fichier CSV d'analyse trouv√©")
            return {"comments": []}
        
        # Si un video_id est fourni, filtrer les fichiers pour cette vid√©o
        if video_id:
            matching_files = [f for f in csv_files if f.startswith(f"youtube_comments_{video_id}_labeled")]
            
            if matching_files:
                # Utiliser le fichier le plus r√©cent pour cette vid√©o
                matching_files.sort(reverse=True)
                latest_csv = os.path.join(csv_dir, matching_files[0])
                logger.info(f"Utilisation du fichier le plus r√©cent pour la vid√©o {video_id}: {matching_files[0]}")
            else:
                # Si aucun fichier ne correspond, v√©rifier s'il existe des fichiers non-labelis√©s pour cette vid√©o
                unlabeled_file = os.path.join(csv_dir, f"youtube_comments_{video_id}.csv")
                if os.path.exists(unlabeled_file):
                    logger.warning(f"Aucun fichier labelis√© trouv√© pour la vid√©o {video_id}, mais un fichier brut existe. Analyse non compl√©t√©e?")
                else:
                    logger.warning(f"Aucun fichier trouv√© pour la vid√©o {video_id}")
                
                # Fallback: utiliser le fichier le plus r√©cent (toutes vid√©os confondues)
                csv_files.sort(reverse=True)
                latest_csv = os.path.join(csv_dir, csv_files[0])
                logger.warning(f"Utilisation du fichier le plus r√©cent comme fallback: {csv_files[0]}")
        else:
            # Si aucun video_id fourni, logique existante pour utiliser le plus r√©cent
            # Am√©lioration: r√©cup√©rer l'ID vid√©o le plus r√©cent (premier fichier tri√©)
            csv_files.sort(reverse=True)
            
            # Extraire l'ID vid√©o du fichier le plus r√©cent
            most_recent_file = csv_files[0]
            extracted_video_id = None
            
            # Extraction de l'ID vid√©o du nom de fichier (youtube_comments_VIDEO_ID_labeled_...)
            if most_recent_file.startswith("youtube_comments_"):
                parts = most_recent_file.split("_labeled_")[0].split("youtube_comments_")
                if len(parts) > 1:
                    extracted_video_id = parts[1]
                    logger.info(f"ID vid√©o d√©tect√© dans le fichier le plus r√©cent: {extracted_video_id}")
            
            # Filtrer les fichiers correspondant √† cet ID vid√©o si disponible
            if extracted_video_id:
                matching_files = [f for f in csv_files if f.startswith(f"youtube_comments_{extracted_video_id}_labeled")]
                if matching_files:
                    # Utiliser le fichier le plus r√©cent pour cet ID vid√©o
                    matching_files.sort(reverse=True)
                    latest_csv = os.path.join(csv_dir, matching_files[0])
                    logger.info(f"Utilisation du fichier le plus r√©cent pour la vid√©o {extracted_video_id}: {matching_files[0]}")
                else:
                    # Si aucun fichier correspondant, utiliser le plus r√©cent g√©n√©ral
                    latest_csv = os.path.join(csv_dir, csv_files[0])
                    logger.warning(f"Aucun fichier ne correspond √† l'ID vid√©o {extracted_video_id}, utilisation du plus r√©cent")
            else:
                # Si on ne peut pas extraire l'ID vid√©o, utiliser le plus r√©cent
                latest_csv = os.path.join(csv_dir, csv_files[0])
                logger.warning("Impossible de d√©terminer l'ID vid√©o, utilisation du fichier le plus r√©cent")
        
        logger.info(f"Lecture des commentaires depuis: {latest_csv}")
        
        comments = []
        if os.path.exists(latest_csv):
            with open(latest_csv, 'r', encoding='utf-8') as file:
                import csv
                reader = csv.reader(file)
                header = next(reader)  # Skip header row
                
                # D√©terminer les indices des colonnes
                text_idx = header.index('text') if 'text' in header else 0
                label_idx = None
                prob_idx = None
                
                for i, col in enumerate(header):
                    if col.lower() == 'label' or col.lower() == 'emotion':
                        label_idx = i
                    elif col.lower() == 'probability' or col.lower() == 'prob' or col.lower() == 'confidence':
                        prob_idx = i
                
                if label_idx is None:
                    logger.warning("Colonne 'label' non trouv√©e dans le CSV")
                    return {"comments": []}
                
                # Lire les lignes et filtrer par √©motion
                for i, row in enumerate(reader):
                    if len(row) <= label_idx:
                        continue  # Ignorer les lignes incompl√®tes
                    
                    if row[label_idx].lower() == emotion.lower():
                        comment = {
                            "id": i,
                            "text": row[text_idx]
                        }
                        
                        # Ajouter la probabilit√© si disponible
                        if prob_idx is not None and len(row) > prob_idx:
                            try:
                                comment["probability"] = float(row[prob_idx])
                            except (ValueError, TypeError):
                                pass
                        
                        comments.append(comment)
        
        logger.info(f"Nombre de commentaires trouv√©s pour l'√©motion '{emotion}': {len(comments)}")
        
        # Limiter le nombre de commentaires retourn√©s pour des raisons de performance
        max_comments = 100
        if len(comments) > max_comments:
            import random
            comments = random.sample(comments, max_comments)
            logger.info(f"Limit√© √† {max_comments} commentaires al√©atoires")
        
        return {"comments": comments}
    
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des commentaires pour l'√©motion '{emotion}': {str(e)}", exc_info=True)
        return {"error": str(e), "comments": []}